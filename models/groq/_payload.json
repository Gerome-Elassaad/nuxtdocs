[{"data":1,"prerenderedAt":471},["ShallowReactive",2],{"navigation":3,"/models/groq":105,"/models/groq-surround":466},[4,28,45,91],{"title":5,"path":6,"stem":7,"children":8,"icon":27},"Getting Started","/getting-started","1.getting-started/1.index",[9,12,17,22],{"title":10,"path":6,"stem":7,"icon":11},"Introduction","i-lucide-house",{"title":13,"path":14,"stem":15,"icon":16},"Quickstart","/getting-started/quickstart","1.getting-started/2.quickstart","i-lucide-zap",{"title":18,"path":19,"stem":20,"icon":21},"Teams","/getting-started/teams","1.getting-started/3.teams","i-lucide-users",{"title":23,"path":24,"stem":25,"icon":26},"Developer Docs","/getting-started/developer-docs","1.getting-started/4.developer-docs","i-heroicons-rocket-launch","i-lucide-rocket",{"title":29,"icon":30,"path":31,"stem":32,"children":33,"page":44},"Integrations","i-lucide-infinity","/integrations","2.integrations",[34,39],{"title":35,"path":36,"stem":37,"icon":38},"GitHub","/integrations/github","2.integrations/1.github","i-simple-icons-github",{"title":40,"path":41,"stem":42,"icon":43},"Plugins","/integrations/plugins","2.integrations/2.plugins","i-lucide-plug-zap",false,{"title":46,"icon":47,"path":48,"stem":49,"children":50,"page":44},"Models","i-lucide-cpu","/models","3.models",[51,56,61,66,71,76,81,86],{"title":52,"path":53,"stem":54,"icon":55},"Anthropic","/models/anthropic","3.models/1.anthropic","i-simple-icons-anthropic",{"title":57,"path":58,"stem":59,"icon":60},"DeepSeek","/models/deepseek","3.models/2.deepseek","i-lucide-search",{"title":62,"path":63,"stem":64,"icon":65},"Google Gemini","/models/gemini","3.models/4.gemini","i-simple-icons-googlegemini",{"title":67,"path":68,"stem":69,"icon":70},"xAI Groq","/models/groq","3.models/5.groq","i-lucide-bot",{"title":72,"path":73,"stem":74,"icon":75},"Mistral","/models/mistral","3.models/6.mistral","i-lucide-wind",{"title":77,"path":78,"stem":79,"icon":80},"Ollama","/models/ollama","3.models/7.ollama","i-lucide-server",{"title":82,"path":83,"stem":84,"icon":85},"OpenAI","/models/openai","3.models/8.openai","i-simple-icons-openai",{"title":87,"path":88,"stem":89,"icon":90},"Vertex AI","/models/gcp-vertex-ai","3.models/9.gcp-vertex-ai","i-simple-icons-google",{"title":92,"icon":93,"path":94,"stem":95,"children":96,"page":44},"Providers","i-lucide-cloud","/providers","4.providers",[97,101],{"title":98,"path":99,"stem":100,"icon":21},"Together AI","/providers/togetherai","4.providers/1.togetherai",{"title":102,"path":103,"stem":104,"icon":16},"Fireworks AI","/providers/fireworks","4.providers/2.fireworks",{"id":106,"title":67,"body":107,"description":459,"extension":460,"links":461,"meta":462,"navigation":463,"path":68,"seo":464,"stem":69,"__hash__":465},"docs/3.models/5.groq.md",{"type":108,"value":109,"toc":448},"minimark",[110,117,120,134,139,181,185,188,193,221,225,245,249,275,279,287,291,317,321,324,328,331,341,350,354,361,375,384,388,412,416],[111,112,113],"blockquote",{},[114,115,116],"p",{},"Learn how to configure and use xAI's Grok models with CodinIT.dev, including API key setup, supported models, and reasoning capabilities.",[114,118,119],{},"xAI is the company behind Grok, a large language model known for its conversational abilities and large context window. Grok models are designed to provide helpful, informative, and contextually relevant responses.",[114,121,122,126,127],{},[123,124,125],"strong",{},"Website:"," ",[128,129,131],"a",{"ariaLabel":130,"href":131,"rel":132},"xAI Website","https://x.ai/",[133],"nofollow",[135,136,138],"h3",{"id":137},"getting-an-api-key","Getting an API Key",[140,141,142,155,161,167],"ol",{},[143,144,145,148,149,154],"li",{},[123,146,147],{},"Sign Up/Sign In:"," Go to the ",[128,150,151],{"ariaLabel":151,"href":152,"rel":153},"xAI Console","https://console.x.ai/",[133],". Create an account or sign in.",[143,156,157,160],{},[123,158,159],{},"Navigate to API Keys:"," Go to the API keys section in your dashboard.",[143,162,163,166],{},[123,164,165],{},"Create a Key:"," Click to create a new API key. Give your key a descriptive name (e.g., \"CodinIT.dev\").",[143,168,169,126,172,175,176,180],{},[123,170,171],{},"Copy the Key:",[123,173,174],{},"Important:"," Copy the API key ",[177,178,179],"em",{},"immediately",". You will not be able to see it again. Store it securely.",[135,182,184],{"id":183},"supported-models","Supported Models",[114,186,187],{},"CodinIT.dev supports the following xAI Grok models:",[189,190,192],"h4",{"id":191},"grok-3-models","Grok-3 Models",[194,195,196,203,209,215],"ul",{},[143,197,198,202],{},[199,200,201],"code",{},"grok-3-beta"," (Default) - xAI's Grok-3 beta model with 131K context window",[143,204,205,208],{},[199,206,207],{},"grok-3-fast-beta"," - xAI's Grok-3 fast beta model with 131K context window",[143,210,211,214],{},[199,212,213],{},"grok-3-mini-beta"," - xAI's Grok-3 mini beta model with 131K context window",[143,216,217,220],{},[199,218,219],{},"grok-3-mini-fast-beta"," - xAI's Grok-3 mini fast beta model with 131K context window",[189,222,224],{"id":223},"grok-2-models","Grok-2 Models",[194,226,227,233,239],{},[143,228,229,232],{},[199,230,231],{},"grok-2-latest"," - xAI's Grok-2 model - latest version with 131K context window",[143,234,235,238],{},[199,236,237],{},"grok-2"," - xAI's Grok-2 model with 131K context window",[143,240,241,244],{},[199,242,243],{},"grok-2-1212"," - xAI's Grok-2 model (version 1212) with 131K context window",[189,246,248],{"id":247},"grok-vision-models","Grok Vision Models",[194,250,251,257,263,269],{},[143,252,253,256],{},[199,254,255],{},"grok-2-vision-latest"," - xAI's Grok-2 Vision model - latest version with image support and 32K context window",[143,258,259,262],{},[199,260,261],{},"grok-2-vision"," - xAI's Grok-2 Vision model with image support and 32K context window",[143,264,265,268],{},[199,266,267],{},"grok-2-vision-1212"," - xAI's Grok-2 Vision model (version 1212) with image support and 32K context window",[143,270,271,274],{},[199,272,273],{},"grok-vision-beta"," - xAI's Grok Vision Beta model with image support and 8K context window",[189,276,278],{"id":277},"legacy-models","Legacy Models",[194,280,281],{},[143,282,283,286],{},[199,284,285],{},"grok-beta"," - xAI's Grok Beta model (legacy) with 131K context window",[135,288,290],{"id":289},"configuration-in-codinitdev","Configuration in CodinIT.dev",[140,292,293,299,305,311],{},[143,294,295,298],{},[123,296,297],{},"Open CodinIT.dev Settings:"," Click the settings icon (⚙️) in the CodinIT.dev panel.",[143,300,301,304],{},[123,302,303],{},"Select Provider:"," Choose \"xAI\" from the \"API Provider\" dropdown.",[143,306,307,310],{},[123,308,309],{},"Enter API Key:"," Paste your xAI API key into the \"xAI API Key\" field.",[143,312,313,316],{},[123,314,315],{},"Select Model:"," Choose your desired Grok model from the \"Model\" dropdown.",[135,318,320],{"id":319},"reasoning-capabilities","Reasoning Capabilities",[114,322,323],{},"Grok 3 Mini models feature specialized reasoning capabilities, allowing them to \"think before responding\" - particularly useful for complex problem-solving tasks.",[189,325,327],{"id":326},"reasoning-enabled-models","Reasoning-Enabled Models",[114,329,330],{},"Reasoning is only supported by:",[194,332,333,337],{},[143,334,335],{},[199,336,213],{},[143,338,339],{},[199,340,219],{},[114,342,343,344,346,347,349],{},"The Grok 3 models ",[199,345,201],{}," and ",[199,348,207],{}," do not support reasoning.",[189,351,353],{"id":352},"controlling-reasoning-effort","Controlling Reasoning Effort",[114,355,356,357,360],{},"When using reasoning-enabled models, you can control how hard the model thinks with the ",[199,358,359],{},"reasoning_effort"," parameter:",[194,362,363,369],{},[143,364,365,368],{},[199,366,367],{},"low",": Minimal thinking time, using fewer tokens for quick responses",[143,370,371,374],{},[199,372,373],{},"high",": Maximum thinking time, leveraging more tokens for complex problems",[114,376,377,378,380,381,383],{},"Choose ",[199,379,367],{}," for simple queries that should complete quickly, and ",[199,382,373],{}," for harder problems where response latency is less important.",[189,385,387],{"id":386},"key-features","Key Features",[194,389,390,396,402],{},[143,391,392,395],{},[123,393,394],{},"Step-by-Step Problem Solving",": The model thinks through problems methodically before delivering an answer",[143,397,398,401],{},[123,399,400],{},"Math & Quantitative Strength",": Excels at numerical challenges and logic puzzles",[143,403,404,407,408,411],{},[123,405,406],{},"Reasoning Trace Access",": The model's thinking process is available via the ",[199,409,410],{},"reasoning_content"," field in the response completion object",[135,413,415],{"id":414},"tips-and-notes","Tips and Notes",[194,417,418,424,436,442],{},[143,419,420,423],{},[123,421,422],{},"Context Window:"," Most Grok models feature large context windows (up to 131K tokens), allowing you to include substantial amounts of code and context in your prompts.",[143,425,426,429,430,432,433,435],{},[123,427,428],{},"Vision Capabilities:"," Select vision-enabled models (",[199,431,255],{},", ",[199,434,261],{},", etc.) when you need to process or analyze images.",[143,437,438,441],{},[123,439,440],{},"Pricing:"," Pricing varies by model, with input costs ranging from $0.3 to $5.0 per million tokens and output costs from $0.5 to $25.0 per million tokens. Refer to the xAI documentation for the most current pricing information.",[143,443,444,447],{},[123,445,446],{},"Performance Tradeoffs:"," \"Fast\" variants typically offer quicker response times but may have higher costs, while \"mini\" variants are more economical but may have reduced capabilities.",{"title":449,"searchDepth":450,"depth":451,"links":452},"",1,2,[453,455,456,457,458],{"id":137,"depth":454,"text":138},3,{"id":183,"depth":454,"text":184},{"id":289,"depth":454,"text":290},{"id":319,"depth":454,"text":320},{"id":414,"depth":454,"text":415},"Complete guide to integrating Groq models with CodinIT.dev for intelligent app development with real-time capabilities.","md",null,{},{"icon":70},{"title":67,"description":459},"gWOclb0oqzD59EQL-W-JQ77TPaky0eQCXLYJBL-h0Dk",[467,469],{"title":62,"path":63,"stem":64,"description":468,"icon":65,"children":-1},"Complete guide to integrating Google Gemini AI models with CodinIT.dev for intelligent app development.",{"title":72,"path":73,"stem":74,"description":470,"icon":75,"children":-1},"Complete guide to integrating Mistral AI models with CodinIT.dev for precise code generation and multilingual development.",1751293534210]