[{"data":1,"prerenderedAt":424},["ShallowReactive",2],{"navigation":3,"/models/ollama":105,"/models/ollama-surround":419},[4,28,45,91],{"title":5,"path":6,"stem":7,"children":8,"icon":27},"Getting Started","/getting-started","1.getting-started/1.index",[9,12,17,22],{"title":10,"path":6,"stem":7,"icon":11},"Introduction","i-lucide-house",{"title":13,"path":14,"stem":15,"icon":16},"Quickstart","/getting-started/quickstart","1.getting-started/2.quickstart","i-lucide-zap",{"title":18,"path":19,"stem":20,"icon":21},"Teams","/getting-started/teams","1.getting-started/3.teams","i-lucide-users",{"title":23,"path":24,"stem":25,"icon":26},"Developer Docs","/getting-started/developer-docs","1.getting-started/4.developer-docs","i-heroicons-rocket-launch","i-lucide-rocket",{"title":29,"icon":30,"path":31,"stem":32,"children":33,"page":44},"Integrations","i-lucide-infinity","/integrations","2.integrations",[34,39],{"title":35,"path":36,"stem":37,"icon":38},"GitHub","/integrations/github","2.integrations/1.github","i-simple-icons-github",{"title":40,"path":41,"stem":42,"icon":43},"Plugins","/integrations/plugins","2.integrations/2.plugins","i-lucide-plug-zap",false,{"title":46,"icon":47,"path":48,"stem":49,"children":50,"page":44},"Models","i-lucide-cpu","/models","3.models",[51,56,61,66,71,76,81,86],{"title":52,"path":53,"stem":54,"icon":55},"Anthropic","/models/anthropic","3.models/1.anthropic","i-simple-icons-anthropic",{"title":57,"path":58,"stem":59,"icon":60},"DeepSeek","/models/deepseek","3.models/2.deepseek","i-lucide-search",{"title":62,"path":63,"stem":64,"icon":65},"Google Gemini","/models/gemini","3.models/4.gemini","i-simple-icons-googlegemini",{"title":67,"path":68,"stem":69,"icon":70},"xAI Groq","/models/groq","3.models/5.groq","i-lucide-bot",{"title":72,"path":73,"stem":74,"icon":75},"Mistral","/models/mistral","3.models/6.mistral","i-lucide-wind",{"title":77,"path":78,"stem":79,"icon":80},"Ollama","/models/ollama","3.models/7.ollama","i-lucide-server",{"title":82,"path":83,"stem":84,"icon":85},"OpenAI","/models/openai","3.models/8.openai","i-simple-icons-openai",{"title":87,"path":88,"stem":89,"icon":90},"Vertex AI","/models/gcp-vertex-ai","3.models/9.gcp-vertex-ai","i-simple-icons-google",{"title":92,"icon":93,"path":94,"stem":95,"children":96,"page":44},"Providers","i-lucide-cloud","/providers","4.providers",[97,101],{"title":98,"path":99,"stem":100,"icon":21},"Together AI","/providers/togetherai","4.providers/1.togetherai",{"title":102,"path":103,"stem":104,"icon":16},"Fireworks AI","/providers/fireworks","4.providers/2.fireworks",{"id":106,"title":77,"body":107,"description":412,"extension":413,"links":414,"meta":415,"navigation":416,"path":78,"seo":417,"stem":79,"__hash__":418},"docs/3.models/7.ollama.md",{"type":108,"value":109,"toc":406},"minimark",[110,114,128,133,360,364,402],[111,112,113],"p",{},"CodinIT.dev supports running models locally using Ollama. This approach offers privacy, offline access, and potentially reduced costs. It requires some initial setup and a sufficiently powerful computer. Because of the present state of consumer hardware, it's not recommended to use Ollama with CodinIT.dev as performance will likely be poor for average hardware configurations.",[111,115,116,120,121],{},[117,118,119],"strong",{},"Website:"," ",[122,123,125],"a",{"ariaLabel":124,"href":125,"rel":126},"Ollama Website","https://ollama.com/",[127],"nofollow",[129,130,132],"h3",{"id":131},"setting-up-ollama","Setting up Ollama",[134,135,136,171,275,328],"ol",{},[137,138,139,142,143,147,148],"li",{},[117,140,141],{},"Download and Install Ollama:","\nObtain the Ollama installer for your operating system from the ",[122,144,146],{"ariaLabel":124,"href":125,"rel":145},[127],"Ollama website"," and follow their installation guide. Ensure Ollama is running. You can typically start it with:",[149,150,155],"pre",{"className":151,"code":152,"language":153,"meta":154,"style":154},"language-bash shiki shiki-themes material-theme-lighter material-theme material-theme-palenight","ollama serve\n","bash","",[156,157,158],"code",{"__ignoreMap":154},[159,160,163,167],"span",{"class":161,"line":162},"line",1,[159,164,166],{"class":165},"sBMFI","ollama",[159,168,170],{"class":169},"sfazB"," serve\n",[137,172,173,176,177,183,184,228,231,232,258,260,261],{},[117,174,175],{},"Download a Model:","\nOllama supports a wide variety of models. A list of available models can be found on the ",[122,178,182],{"ariaLabel":179,"href":180,"rel":181},"Ollama Model Library","https://ollama.com/library",[127],"Ollama model library",". Some models recommended for coding tasks include:",[185,186,187,193,199,205,210,216,222],"ul",{},[137,188,189,192],{},[156,190,191],{},"codellama:7b-code"," (a good, smaller starting point)",[137,194,195,198],{},[156,196,197],{},"codellama:13b-code"," (offers better quality, larger size)",[137,200,201,204],{},[156,202,203],{},"codellama:34b-code"," (provides even higher quality, very large)",[137,206,207],{},[156,208,209],{},"qwen2.5-coder:32b",[137,211,212,215],{},[156,213,214],{},"mistralai/Mistral-7B-Instruct-v0.1"," (a solid general-purpose model)",[137,217,218,221],{},[156,219,220],{},"deepseek-coder:6.7b-base"," (effective for coding)",[137,223,224,227],{},[156,225,226],{},"llama3:8b-instruct-q5_1"," (suitable for general tasks)",[229,230],"br",{},"To download a model, open your terminal and execute:",[149,233,235],{"className":151,"code":234,"language":153,"meta":154,"style":154},"ollama pull \u003Cmodel_name>\n",[156,236,237],{"__ignoreMap":154},[159,238,239,241,244,248,251,255],{"class":161,"line":162},[159,240,166],{"class":165},[159,242,243],{"class":169}," pull",[159,245,247],{"class":246},"sMK4o"," \u003C",[159,249,250],{"class":169},"model_nam",[159,252,254],{"class":253},"sTEyZ","e",[159,256,257],{"class":246},">\n",[229,259],{},"For instance:",[149,262,264],{"className":151,"code":263,"language":153,"meta":154,"style":154},"ollama pull qwen2.5-coder:32b\n",[156,265,266],{"__ignoreMap":154},[159,267,268,270,272],{"class":161,"line":162},[159,269,166],{"class":165},[159,271,243],{"class":169},[159,273,274],{"class":169}," qwen2.5-coder:32b\n",[137,276,277,280,281,283,284,286,287,301,303,304,312,314,315,321,323,324,327],{},[117,278,279],{},"Configure the Model's Context Window:","\nBy default, Ollama models often use a context window of 2048 tokens, which can be insufficient for many CodinIT.dev requests. A minimum of 12,000 tokens is advisable for decent results, with 32,000 tokens being ideal. To adjust this, you'll modify the model's parameters and save it as a new version.",[229,282],{},"First, load the model (using ",[156,285,209],{}," as an example):",[149,288,290],{"className":151,"code":289,"language":153,"meta":154,"style":154},"ollama run qwen2.5-coder:32b\n",[156,291,292],{"__ignoreMap":154},[159,293,294,296,299],{"class":161,"line":162},[159,295,166],{"class":165},[159,297,298],{"class":169}," run",[159,300,274],{"class":169},[229,302],{},"Once the model is loaded within the Ollama interactive session, set the context size parameter:",[149,305,310],{"className":306,"code":308,"language":309},[307],"language-text","/set parameter num_ctx 32768\n","text",[156,311,308],{"__ignoreMap":154},[229,313],{},"Then, save this configured model with a new name:",[149,316,319],{"className":317,"code":318,"language":309},[307],"/save your_custom_model_name\n",[156,320,318],{"__ignoreMap":154},[229,322],{},"(Replace ",[156,325,326],{},"your_custom_model_name"," with a name of your choice.)",[137,329,330,333],{},[117,331,332],{},"Configure CodinIT.dev:",[185,334,335,338,341,344,350,357],{},[137,336,337],{},"Open the CodinIT.dev sidebar (usually indicated by the CodinIT.dev icon).",[137,339,340],{},"Click the settings gear icon (⚙️).",[137,342,343],{},"Select \"ollama\" as the API Provider.",[137,345,346,347,349],{},"Enter the Model name you saved in the previous step (e.g., ",[156,348,326],{},").",[137,351,352,353,356],{},"(Optional) Adjust the base URL if Ollama is running on a different machine or port. The default is ",[156,354,355],{},"http://localhost:11434",".",[137,358,359],{},"(Optional) Configure the Model context size in CodinIT.dev's Advanced settings. This helps CodinIT.dev manage its context window effectively with your customized Ollama model.",[129,361,363],{"id":362},"tips-and-notes","Tips and Notes",[185,365,366,372,378,384,390],{},[137,367,368,371],{},[117,369,370],{},"Resource Demands:"," Running large language models locally can be demanding on system resources. Ensure your computer meets the requirements for your chosen model.",[137,373,374,377],{},[117,375,376],{},"Model Choice:"," Experiment with various models to discover which best fits your specific tasks and preferences.",[137,379,380,383],{},[117,381,382],{},"Offline Capability:"," After downloading a model, you can use CodinIT.dev with that model even without an internet connection.",[137,385,386,389],{},[117,387,388],{},"Token Usage Tracking:"," CodinIT.dev tracks token usage for models accessed via Ollama, allowing you to monitor consumption.",[137,391,392,395,396,356],{},[117,393,394],{},"Ollama's Own Documentation:"," For more detailed information, consult the official ",[122,397,401],{"ariaLabel":398,"href":399,"rel":400},"Ollama Documentation","https://ollama.com/docs",[127],"Ollama documentation",[403,404,405],"style",{},"html pre.shiki code .sBMFI, html code.shiki .sBMFI{--shiki-light:#E2931D;--shiki-default:#FFCB6B;--shiki-dark:#FFCB6B}html pre.shiki code .sfazB, html code.shiki .sfazB{--shiki-light:#91B859;--shiki-default:#C3E88D;--shiki-dark:#C3E88D}html .light .shiki span {color: var(--shiki-light);background: var(--shiki-light-bg);font-style: var(--shiki-light-font-style);font-weight: var(--shiki-light-font-weight);text-decoration: var(--shiki-light-text-decoration);}html.light .shiki span {color: var(--shiki-light);background: var(--shiki-light-bg);font-style: var(--shiki-light-font-style);font-weight: var(--shiki-light-font-weight);text-decoration: var(--shiki-light-text-decoration);}html .default .shiki span {color: var(--shiki-default);background: var(--shiki-default-bg);font-style: var(--shiki-default-font-style);font-weight: var(--shiki-default-font-weight);text-decoration: var(--shiki-default-text-decoration);}html .shiki span {color: var(--shiki-default);background: var(--shiki-default-bg);font-style: var(--shiki-default-font-style);font-weight: var(--shiki-default-font-weight);text-decoration: var(--shiki-default-text-decoration);}html .dark .shiki span {color: var(--shiki-dark);background: var(--shiki-dark-bg);font-style: var(--shiki-dark-font-style);font-weight: var(--shiki-dark-font-weight);text-decoration: var(--shiki-dark-text-decoration);}html.dark .shiki span {color: var(--shiki-dark);background: var(--shiki-dark-bg);font-style: var(--shiki-dark-font-style);font-weight: var(--shiki-dark-font-weight);text-decoration: var(--shiki-dark-text-decoration);}html pre.shiki code .sMK4o, html code.shiki .sMK4o{--shiki-light:#39ADB5;--shiki-default:#89DDFF;--shiki-dark:#89DDFF}html pre.shiki code .sTEyZ, html code.shiki .sTEyZ{--shiki-light:#90A4AE;--shiki-default:#EEFFFF;--shiki-dark:#BABED8}",{"title":154,"searchDepth":162,"depth":407,"links":408},2,[409,411],{"id":131,"depth":410,"text":132},3,{"id":362,"depth":410,"text":363},"Complete guide to integrating Ollama local AI models with CodinIT.dev for privacy-first development.","md",null,{},{"icon":80},{"title":77,"description":412},"cvFcY4lG0JgPgC8top4ksL4p4r4SBg2xW0R4qGryE5c",[420,422],{"title":72,"path":73,"stem":74,"description":421,"icon":75,"children":-1},"Complete guide to integrating Mistral AI models with CodinIT.dev for precise code generation and multilingual development.",{"title":82,"path":83,"stem":84,"description":423,"icon":85,"children":-1},"Complete guide to integrating OpenAI GPT models with CodinIT.dev for AI-powered application development.",1751293533493]